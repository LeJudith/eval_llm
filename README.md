
# Evaluation Metrics for Large Language Models (LLMs)

This tool evaluates the performance of Large Language Models (LLMs) using various metrics to quantify the similarity and quality of generated texts compared to reference texts. It supports multiple metrics, including Jaccard Index, cosine similarity, BERTScore, and machine translation metrics, tailored to compare biomedical or domain-specific texts. 

## Features
- Calculate a range of evaluation metrics, including keyword-based, similarity-based, and model-based scoring methods.
- Use a custom keyword dictionary to adjust keyword sensitivity.
- Input files can be in CSV or JSON format.
- Saves metric results in a user-defined JSON output file.

## Installation
Ensure you have Python and necessary dependencies installed. Install required packages using:
```bash
pip install -r requirements.txt
```

## Usage

### Command Line Arguments 

| Argument       | Description                                                                                                      | Required |
|----------------|------------------------------------------------------------------------------------------------------------------|----------|
| `input_file`   | Path to the input file (CSV or JSON) containing the texts to evaluate.                                           | Yes      |
| `output_dir`  | Path to the output directory where the computed metrics will be stored.                                           | Yes      |
| `dict_path`    | (Optional) Path to a custom JSON keyword dictionary for keyword-based metrics. Defaults to PALGA thesaurus       | No       |
| `metrics`      | List of metrics to compute. Available options: `jaccard_index_dictionary`, `jaccard_index_scispacy`, `cosine_similarity_biobert`, `machine_translation_metrics`, `bert_score_metric`. Defaults to all metrics if not specified. | No     |

### Input File Format
The input file should contain two columns labeled `original` and `generated`. The `original` column contains reference texts, and the `generated` column contains the corresponding texts generated by the LLM. The input file can be in CSV or JSON format.

#### Example CSV Format
```csv
original,generated
original_text1,generated_text1
original_text2,generated_text2
```

#### Example JSON Format
```json
{
  "original": ["original_text1", "original_text2"],
  "generated": ["generated_text1", "generated_text2"]
}

```

### Available Metrics

- **`jaccard_index_dictionary`**: Calculates the Jaccard Index based on a custom dictionary of keywords.
- **`jaccard_index_scispacy`**: Uses the SciSpacy model to calculate the Jaccard Index, focusing on biomedical entities.
- **`cosine_similarity_biobert`**: Measures cosine similarity using BioBERT embeddings, suitable for biomedical text comparisons.
- **`machine_translation_metrics`**: Evaluates using machine translation metrics like BLEU, ROUGE, CIDER,METEOR, commonly used in translation quality assessment.
- **`bert_score_metric`**: Computes BERTScore, which uses contextual embeddings for a more semantic similarity measure.

### Example Command

To run the evaluation with all metrics:
```bash
python evaluate.py --input_file path/to/input.csv --output_dir path/to/output
```

To specify metrics and use a custom dictionary:

```bash
python evaluate.py --input_file path/to/input.json --output_dir path/to/output.csv --dict_path path/to/dictionary.json --metrics jaccard_index_dictionary bert_score_metric 
```

## Output
The output file will contain the results for each metric computed on each input text pair, stored in CSV or JSON format, depending on your choice. Each row will correspond to a text pair with the computed metric scores.

## Dependencies
- Python 3.x
- SpaCy
- SciSpacy
- BioBERT and other relevant models from Hugging Face's `transformers` library
- BERTScore, ROUGE, BLEU (for machine translation metrics)

Install dependencies by running:
```bash
pip install -r requirements.txt
```

## License
This project is licensed under the MIT License.

